# LLM_App_using_Local_Ollama_model
Build simple LLM application using locally host llama3.2:1b model from ollama and trace the application activities using LangSmith finally make a simple UI using Streamlit
